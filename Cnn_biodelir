# -*- coding: utf-8 -*-
"""
Created on Mon Jul  6 08:50:37 2020
@author: beurier, cornet, rouan
"""
import sys
import Spectrum_load as Loader 
import Spectrum_filter as Filters
import Spectrum_noise as Noise
import Spectrum_split as Split
from tensorflow.keras.models import Sequential
from tensorflow.keras import models  
from tensorflow.keras.layers import Dense, Conv1D, Flatten, BatchNormalization, SpatialDropout1D, LeakyReLU, Bidirectional, LSTM
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras import backend as K
from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import r2_score
import tensorflow as tf
import pandas as pd
import numpy as np
import random
import warnings
import os
from pandas import read_csv, concat
import matplotlib.pyplot as plt
from datetime import date, datetime
from csv import writer
from sklearn.exceptions import DataConversionWarning, ConvergenceWarning
warnings.filterwarnings(action='ignore', category=DataConversionWarning)
warnings.filterwarnings(action='ignore', category=FutureWarning)
warnings.filterwarnings(action='ignore', category=UserWarning)
warnings.filterwarnings(action='ignore', category=ConvergenceWarning)

## command parameters
GPU_ID = '0'
os.environ["CUDA_VISIBLE_DEVICES"] = GPU_ID  # The GPU id to use
SIGNAL = ''
SPLIT = False
TESTPROP=0.25
NOISE = [True, 'KeepDist', 3] # 3 args: True/False, UniDist/KeepDist, Nb Noised Spectra By Original Spectra 
FILTER = True
FILTER_DEPTH = 1
REP = 1 # number of rep of spectra by analyte value


## Loading options
DECIMAL="."
HEADER = None
DTYPE = np.float32

## PARAMS
SCALERS_RANGE = (0.1, 0.9)
NB_EPOCHS = 5000
BATCH_SIZE = 64
K_FOLD_NB = 3
K_FOLD_REPEAT = 1
SEED = 84269713
DROPOUT_RATE= 0.05
REGULARIZATION = (0, 0, 1) # 0 = BatcNorm; 1=DropOut
ACTIVATION=['selu', 'relu', 'elu', 'softmax', 'linear'] # softmax linear   default:'selu', 'relu', 'elu', 'sigmoid', 'sigmoid'
OPTIMIZER = 'rmsprop' 

lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.1)

today = datetime.now()
today = today.strftime("%d-%m-%Y_%H-%M-%S")

## Random seed
def set_seed(sd):
    """ Initialize random seed in python, numpy and tensorflow """
    random.seed(sd)
    np.random.seed(sd)

def load_data():
    """ Load and format data """
    ## Load csv
    if (NOISE[0]==True):
        x=read_csv('./data/' + FileName + '/split/' + NOISE[1] + '_Xcal.csv', header=None, sep=r'\,|\;', dtype=DTYPE, engine='python')
        xp=read_csv('./data/' + FileName + '/split/' + 'Xval.csv', header=None, sep=r'\,|\;', dtype=DTYPE, engine='python')
        y=read_csv('./data/' + FileName + '/split/' + NOISE[1] + '_Ycal.csv', header=None, sep=r'\,|\;', skip_blank_lines=False, dtype=DTYPE, engine='python')
        yp=read_csv('./data/' + FileName + '/split/' + 'Yval.csv', header=None, sep=r'\,|\;', skip_blank_lines=False, dtype=DTYPE, engine='python')
        y_original = np.copy(y)

    else:
        x=read_csv('./data/' + FileName + '/split/Xcal.csv', header=None, sep=r'\,|\;', dtype=DTYPE, engine='python')
        xp=read_csv('./data/' + FileName + '/split/Xval.csv', header=None, sep=r'\,|\;', dtype=DTYPE, engine='python')
        y=read_csv('./data/' + FileName + '/split/Ycal.csv', header=None, sep=r'\,|\;', skip_blank_lines=False, dtype=DTYPE, engine='python')
        yp=read_csv('./data/' + FileName + '/split/Yval.csv', header=None, sep=r'\,|\;', skip_blank_lines=False, dtype=DTYPE, engine='python')
        y_original = np.copy(y)

    ## Filter Data
    if FILTER == True:
        [x, xp] = Filters.apply_all_filters([x, xp], FILTER_DEPTH)
    else:
        [x, xp] = Filters.apply_solo_filter([x, xp])
    
    
    ## Normalise data
    [x, xp] = Loader.cpl_scale_2D([x, xp], SCALERS_RANGE)
    x_original_norm = np.copy(x)
    y, std_scaler, rob_scaler = Loader.cpl_scale_1D(y.iloc[:,1], SCALERS_RANGE)

    return x, xp, y, std_scaler, rob_scaler, x_original_norm, y_original

def create_model(data):
    """ Create previously hyperparametrized convolutional model """
    feature_count = data.shape[1]
    dimension = data.shape[2]

    # NIRS HYPERIZED GLOBAL
    model = Sequential()
    model.add(SpatialDropout1D(0.08, input_shape=(feature_count, dimension), seed=SEED))
    model.add(Conv1D (filters=8, kernel_size=21, strides=5, activation=ACTIVATION[0]))
    if (REGULARIZATION[0]==0):
        model.add(BatchNormalization())
    else:
        model.add(SpatialDropout1D(DROPOUT_RATE))
    model.add(Conv1D (filters=64, kernel_size=21, strides=3, activation=ACTIVATION[1]))
    if (REGULARIZATION[1]==0):
        model.add(BatchNormalization())
    else:
        model.add(SpatialDropout1D(DROPOUT_RATE))
    # model.add(Bidirectional(LSTM(500)))
    model.add(Conv1D (filters=32, kernel_size=5, strides=3, activation=ACTIVATION[2]))
    if (REGULARIZATION[2]==0):
        model.add(BatchNormalization())
    else:
        model.add(SpatialDropout1D(DROPOUT_RATE))
    model.add(Flatten())
    model.add(Dense(16, activation=ACTIVATION[3]))
    model.add(Dense(1, activation=ACTIVATION[4]))

    return model

def run_cv():
    """ Run the training and return prediction """
    ## Get data
    x, xp, y, _, rob_scaler, x_original_norm, y_original = load_data()
    print("y")
    print(y.shape)
    print("y_original")
    print(y_original.shape)
    ## Run KFold training
    kfold = RepeatedKFold(n_splits=K_FOLD_NB, n_repeats=K_FOLD_REPEAT, random_state=SEED)
    fold = 0
    for train, test in kfold.split(y):
        # config fold
        prefix = './out/pred/BACON_' + FileName + 'filt' + str(FILTER_DEPTH)
        model_filepath = './out/model/BACON_' + FileName+ 'filt' + str(FILTER_DEPTH) + '_fold' + str(fold) + today + '.hdf5'
        prediction_filepath = prefix + '_fold' + str(fold) + today+ '_valid_prediction.csv'
        global_prediction_filepath = prefix + '_fold' + str(fold) + today+ '_calib_prediction'

        ## Config callbacks (adaptative learning rate, early stopping, saving) and optimizer
        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=100, verbose=1, min_delta=0.5e-5, 
                                           mode='min')
        earlyStopping = EarlyStopping(monitor='val_loss', patience=250, verbose=0, mode='min') # TODO test restore_best_weights=True (Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.)
        mcp_save = ModelCheckpoint(model_filepath, save_best_only=True, monitor='val_loss', mode='min') 

        # fit model
        model = create_model(x)
        model.compile(loss='mean_squared_error', metrics=['mae','mse'], optimizer=OPTIMIZER) # loss='mean_squared_error' mean_squared_logarithmic_error huber
        model.fit(x[train], y[train], 
                    epochs=NB_EPOCHS, 
                    batch_size=BATCH_SIZE, 
                    shuffle=True, 
                    validation_data = (x[test], y[test]),
                    verbose=1, 
                    callbacks=[reduce_lr_loss, earlyStopping, mcp_save])

        # load best and evaluate RMSE
        print("Fold n°" + str(fold) + " done.")
        model.load_weights(model_filepath)
        
        predict_test = model.predict(x[test])
        predict_test = rob_scaler.inverse_transform(predict_test)
        y_test = rob_scaler.inverse_transform(y[test].reshape((len(y[test]),1)))
        RMSE = np.sqrt(np.mean((y_test-predict_test)**2))
        print("*** TEST SET RMSE *** > %.3f" % (RMSE))

        y_predict = model.predict(x_original_norm)
        y_predict = rob_scaler.inverse_transform(y_predict)
        RMSE = np.sqrt(np.mean((y_original[:,1]-y_predict)**2))
        np.savetxt(global_prediction_filepath + ".csv", np.c_[y_original, y_predict], delimiter=";")
        print("*** GLOBAL RMSE *** > %.3f" % (RMSE))

        # predict unknown data
        predict = model.predict(xp)
        predict = rob_scaler.inverse_transform(predict)
        np.savetxt(prediction_filepath, predict, delimiter=";")
        
        fold = fold + 1
        
filesList= set(os.listdir("./data/"))
print(filesList)
#l1 = [k for k in l if 'Xcal' in k]
#l2 = [k for k in l if 'Ycal' in k]
#l3 = [k for k in l if 'Yval' in k]
#l4 = [k for k in l if 'Xval' in k]
#filesList=set(l)-set(l1)-set(l2)-set(l3)-set(l4)

res=read_csv('./out/BaconRunResults.csv', sep=';', header=0, dtype='U', engine='python')
resl=res.Filename.unique()
print(resl)
#resl=[word.rstrip("_") for word in resl]
#resl=[word+(".csv") for word in resl]

resl=set(resl)
filesList=filesList-resl
print(filesList)

first = 0

for adb in sorted(filesList):
    first += 1 
    adb=adb.replace(".csv", "")
    FileName = adb
    
    if (SPLIT==True):
        Split.DataSplit("./data/"+FileName+".csv", rep=REP, TestProp=TESTPROP)
        FileName = FileName + "_"
    else:
        # Load data to add row index to ycal and yval + delete empty row
        xcal = pd.read_table('./data/' + FileName + '/raw/Xcal.csv', header=None, sep=';')
        ycal = pd.read_table('./data/' + FileName + '/raw/Ycal.csv', header=None, sep=';')
        xval = pd.read_table('./data/' + FileName + '/raw/Xval.csv', header=None, sep=';')
        yval = pd.read_table('./data/' + FileName + '/raw/Yval.csv', header=None, sep=';')
        xy_cal=pd.concat([ycal, xcal], axis=1)
        xy_val=pd.concat([yval, xval], axis=1)
        xy_cal.insert(loc=0, column='ID', value=xy_cal.index)
        xy_val.insert(loc=0, column='ID', value=xy_val.index)
        xy_cal = xy_cal.dropna()
        xy_val = xy_val.dropna()
        xcal=xy_cal.iloc[:, 2:].values
        ycal=xy_cal.iloc[:, 0:2].values
        xval=xy_val.iloc[:, 2:].values
        yval=xy_val.iloc[:, 0:2].values
        np.savetxt('./data/' + FileName + '/split/Xcal.csv', xcal, delimiter=";")
        np.savetxt('./data/' + FileName + '/split/Ycal.csv', ycal, delimiter=";")
        np.savetxt('./data/' + FileName + '/split/Xval.csv', xval, delimiter=";")
        np.savetxt('./data/' + FileName + '/split/Yval.csv', yval, delimiter=";")
    
    if (NOISE[0]==True):
        Noise.DataAugment(FileName, NOISE[1], NOISE[2], HEADER)
        
    set_seed(SEED)
    run_cv()
    
    # Check predictions for calibration 
    ResPredKfoldCal=[]
    for i in range(K_FOLD_NB):
        y_scores_i=read_csv('./out/pred/BACON_' + FileName + 'filt' + str(FILTER_DEPTH) + '_fold' + str(i) + today + '_calib_prediction.csv', 
                            sep=r'\,|\;', header=None, engine='python')
        ResPredKfoldCal.append(y_scores_i) 
        
    df_concat = concat(ResPredKfoldCal)
    by_row_index = df_concat.groupby(df_concat.index)
    ycal = by_row_index.mean()
    
    rmse_cal=round(np.sqrt(np.mean((ycal[1]-ycal[2])**2)),3)
    r2_cal = round(r2_score(ycal[1], ycal[2]),3)
    
    # Check predictions for validation 
    yval_obs = read_csv('./data/' + FileName + '/raw/Yval.csv', header=None, sep=r'\,|\;', skip_blank_lines=False, dtype=DTYPE, engine='python')
    #yval_obs = np.delete(yval_obs.values, 0, 1)
    
    ResPredKfoldVal=[]
    for i in range(K_FOLD_NB):
        y_scores_i=read_csv('./out/pred/BACON_' + FileName + 'filt' + str(FILTER_DEPTH) + '_fold' + str(i) + today + '_valid_prediction.csv', 
                            sep=r'\,|\;', header=None, engine='python')
        ResPredKfoldVal.append(y_scores_i) 
    
    df_concat = concat(ResPredKfoldVal)
    by_row_index = df_concat.groupby(df_concat.index)
    yval_pred = by_row_index.mean() 
    yval_pred =yval_pred.values.reshape(len(yval_pred),1)
    
    rmse_val=round(np.sqrt(np.mean((yval_obs-yval_pred)**2)),3)
    r2_val = round(r2_score(yval_obs, yval_pred),3)
    resid_cal=ycal[2]-ycal[1]
    resid_val=yval_pred-yval_obs
    
    # Get graph
    plt.rcParams["figure.figsize"] = (15,18)
    plt.subplot(2, 2, 1)
    plt.suptitle(FileName + ", Filter" + str(FILTER) + str(FILTER_DEPTH) + ", Noise" + str(NOISE))
    plt.scatter(ycal[1], ycal[2], c = 'g', alpha=0.1)
    plt.text(np.min(ycal[1])+0.05*(np.max(ycal[1])-np.min(ycal[1])), 
             np.max(ycal[2]), "RMSE = " + str(rmse_cal) + "\n" + "R² = " + str(r2_cal))
    plt.title('Cross-validation', size=16, c="g")
    plt.plot(np.linspace(0,np.max(ycal[1])), np.linspace(0,np.max(ycal[1])), color="black", ls="--")
    plt.axvline(x=np.max(ycal[1]), c='b')
    plt.axvline(x=np.min(ycal[1]), c='b')
    plt.text(np.max(ycal[1])-(np.max(ycal[1])-np.min(ycal[1]))/2, np.min(ycal[2])-0.05*(np.max(ycal[2])-np.min(ycal[2])), 
             "Calibration range : " + str(round(np.min(ycal[1]), 2)) + " - " + str(round(np.max(ycal[1]), 2)), 
             c="b", size=16, ha="center", va="center")
    plt.xlim(np.min(ycal[1])-0.1*(np.max(ycal[1])-np.min(ycal[1])), np.max(ycal[1])+0.1*(np.max(ycal[1])-np.min(ycal[1])))
    plt.ylim(np.min(ycal[2])-0.1*(np.max(ycal[2])-np.min(ycal[2])), np.max(ycal[2])+0.1*(np.max(ycal[2])-np.min(ycal[2])))
    
    plt.subplot(2, 2, 2)
    plt.scatter(yval_obs, yval_pred, c = 'red', alpha=0.1)
    plt.text(np.min(yval_obs)+0.05*(np.max(yval_obs)-np.min(yval_obs)), 
             np.max(yval_pred), "RMSE = " + str(rmse_val) + "\n" + "R² = " + str(r2_val))
    plt.title('Validation', size=16, color="red")
    plt.plot(np.linspace(0,np.max(yval_obs)), np.linspace(0,np.max(yval_obs)), color="black", ls="--")
    plt.axvline(x=np.max(yval_obs[0]), c='b')
    plt.axvline(x=np.min(yval_obs[0]), c='b')
    plt.text(np.max(yval_obs)-(np.max(yval_obs)-np.min(yval_obs))/2, np.min(yval_pred)-0.05*(np.max(yval_pred)-np.min(yval_pred)), 
             "Validation range : " + str(round(np.min(yval_obs), 2)) + " - " + str(round(np.max(yval_obs), 2)), 
             c="b", size=16, ha="center", va="center")
    plt.xlim(np.min(yval_obs[0])-0.1*(np.max(yval_obs[0])-np.min(yval_obs[0])), np.max(yval_obs[0])+0.1*(np.max(yval_obs[0])-np.min(yval_obs[0])))
    plt.ylim(np.min(yval_pred)-0.1*(np.max(yval_pred)-np.min(yval_pred)), np.max(yval_pred)+0.1*(np.max(yval_pred)-np.min(yval_pred)))
    
    # Plot residues
    plt.subplot(2, 2, 3)
    plt.scatter(ycal[1], resid_cal, color="g", alpha=0.1)
    plt.xlim(np.min(ycal[1])-0.1*(np.max(ycal[1])-np.min(ycal[1])), np.max(ycal[1])+0.1*(np.max(ycal[1])-np.min(ycal[1])))
    plt.ylim(np.min(resid_cal)-0.1*(np.max(resid_cal)-np.min(resid_cal)), np.max(resid_cal)+0.1*(np.max(resid_cal)-np.min(resid_cal)))
    plt.axvline(x=np.max(ycal[1]), c='b')
    plt.axvline(x=np.min(ycal[1]), c='b')
    plt.axhline(y=0, c='k', ls=":")
    
    plt.subplot(2, 2, 4)
    plt.scatter(yval_obs, resid_val, color="r", alpha=0.1)
    plt.xlim(np.min(yval_obs[0])-0.1*(np.max(yval_obs[0])-np.min(yval_obs[0])), np.max(yval_obs[0])+0.1*(np.max(yval_obs[0])-np.min(yval_obs[0])))
    plt.ylim(np.min(resid_val[0])-0.1*(np.max(resid_val[0])-np.min(resid_val[0])), np.max(resid_val[0])+0.1*(np.max(resid_val[0])-np.min(resid_val[0])))
    plt.axvline(x=np.max(yval_obs[0]), c='b')
    plt.axvline(x=np.min(yval_obs[0]), c='b')
    plt.axhline(y=0, c='k', ls=":")
    
    plt.savefig('./out/plot/'+ FileName + "Filter" + str(FILTER) + str(FILTER_DEPTH) + "_Noise" + str(NOISE) +  "_Batch" + str(BATCH_SIZE) + "_Epochs" + str(NB_EPOCHS) + today + '_PredObs.png', bbox_inches='tight', dpi=80)
    
    # Save perf in csv file   
    model = models.load_model("./out/model/BACON_" +  FileName + 'filt' + str(FILTER_DEPTH) + '_fold' + str(i) + today + '.hdf5')
    LAYERS = model.layers
    Splitt = SPLIT # to do get the KSindex    
    Res=[today, FileName, len(ycal[0]), len(yval_obs), FILTER, FILTER_DEPTH, NOISE[0], NOISE[1], NOISE[2], SCALERS_RANGE[0], SCALERS_RANGE[1], NB_EPOCHS, BATCH_SIZE, K_FOLD_NB, OPTIMIZER, REGULARIZATION, DROPOUT_RATE, SEED, Splitt, str(tf.__version__), str(tf.__version__), LAYERS, ACTIVATION, rmse_cal, r2_cal, rmse_val, r2_val]
    
    def append_list_as_row(file_name, list_of_elem):
        # Open file in append mode
        with open(file_name, 'a+', newline='') as write_obj:
            # Create a writer object from csv module
            csv_writer = writer(write_obj, delimiter=";")
            # Add contents of list as last row in the csv file
            csv_writer.writerow(list_of_elem)
    append_list_as_row('./out/BaconRunResults.csv', Res)
    plt.close('all')

